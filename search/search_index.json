{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"PyDS: A wrapper for creating, configuring, and managing your data science projects Why this project exists? We started by asking the simple questions: Why do I have to memorize 4 bash incantations in order to release a Python package? Why do I have to remember so many sequences of commands to do anything? What is the kind of tooling that we need to support making \"good\" workflows easy? PyDS was born out in response to these questions. We'd rather avoid the frustration of memorizing commands from a smattering of tools and repetitively recalling a particular folder structure from memory in order to set up my projects and perform common tasks (such as Python package publishing). PyDS follows the philosophy that in order for data scientists to be efficient, they must have tooling at hand that automates the mundane, reduces the number of commands that they need to remember, and makes the sane things easy to do (that's riff off security folks' mantra, \"making the right things easy to do\"). In the spirit of automation, this project was thus born. With it, my aim here is to bring sanity to project initialization. Quickstart Ensure that you have the Anaconda distribution of Python installed, and that conda can be found using your PATH environment variable. Then, install from PyPI: pip install pyds For more information, take a look at the CLI page to see what commands exist! Design philosophy PyDS wraps workflows . Workflows are verbs that, underneath the hood, are implemented by a chain of shell commands. To read more, see the Design Philosophy page for more details. Contributing To learn how to contribute, head over to the Contributing page. Inspirations PyDS is inspired by a lot of conversations and reading others' work. I would like to acknowledge their ideas. Cookiecutter Data Science Cookiecutter Data Science (CDS) provided a great starting point for the directory structure. There are places we deviate from CDS, such as omitting a data/ directory, because in the cloud age, we should be securely referencing single sources of truth for our data by way of URIs, s3 buckets (or compatible), database connections, and more. (My opinion is that data should not live in a project source repository.) Without CDS, the inspiration for automation would not have existed. Data Science Bootstrap Notes This is my online book in which I documented a lot of the workflows and best practices that I developed over my career as a data scientist. It has some deficiencies, however, including a focus on tools , with insufficient focus on workflows . With PyDS, my goal is to bring the focus back on workflows . How to organize your data science project Many years ago (in 2017, to be precise), I wrote down my first ideas on the theme of \"good data science project organization\". The result was a GitHub gist with a lot of ideas, but not automation provided. The Good Research Code Handbook This is an excellent resource that I got wind of in December 2021. In it is a detailed handbook-style resource that lays out step-by-step instructions for structuring your data science and/or research project code. Conversations with colleagues at Moderna My conversations with colleagues on the DSAI team at Moderna were highly informative for this project.","title":"PyDS: A wrapper for creating, configuring, and managing your data science projects"},{"location":"#pyds-a-wrapper-for-creating-configuring-and-managing-your-data-science-projects","text":"","title":"PyDS: A wrapper for creating, configuring, and managing your data science projects"},{"location":"#why-this-project-exists","text":"We started by asking the simple questions: Why do I have to memorize 4 bash incantations in order to release a Python package? Why do I have to remember so many sequences of commands to do anything? What is the kind of tooling that we need to support making \"good\" workflows easy? PyDS was born out in response to these questions. We'd rather avoid the frustration of memorizing commands from a smattering of tools and repetitively recalling a particular folder structure from memory in order to set up my projects and perform common tasks (such as Python package publishing). PyDS follows the philosophy that in order for data scientists to be efficient, they must have tooling at hand that automates the mundane, reduces the number of commands that they need to remember, and makes the sane things easy to do (that's riff off security folks' mantra, \"making the right things easy to do\"). In the spirit of automation, this project was thus born. With it, my aim here is to bring sanity to project initialization.","title":"Why this project exists?"},{"location":"#quickstart","text":"Ensure that you have the Anaconda distribution of Python installed, and that conda can be found using your PATH environment variable. Then, install from PyPI: pip install pyds For more information, take a look at the CLI page to see what commands exist!","title":"Quickstart"},{"location":"#design-philosophy","text":"PyDS wraps workflows . Workflows are verbs that, underneath the hood, are implemented by a chain of shell commands. To read more, see the Design Philosophy page for more details.","title":"Design philosophy"},{"location":"#contributing","text":"To learn how to contribute, head over to the Contributing page.","title":"Contributing"},{"location":"#inspirations","text":"PyDS is inspired by a lot of conversations and reading others' work. I would like to acknowledge their ideas.","title":"Inspirations"},{"location":"#cookiecutter-data-science","text":"Cookiecutter Data Science (CDS) provided a great starting point for the directory structure. There are places we deviate from CDS, such as omitting a data/ directory, because in the cloud age, we should be securely referencing single sources of truth for our data by way of URIs, s3 buckets (or compatible), database connections, and more. (My opinion is that data should not live in a project source repository.) Without CDS, the inspiration for automation would not have existed.","title":"Cookiecutter Data Science"},{"location":"#data-science-bootstrap-notes","text":"This is my online book in which I documented a lot of the workflows and best practices that I developed over my career as a data scientist. It has some deficiencies, however, including a focus on tools , with insufficient focus on workflows . With PyDS, my goal is to bring the focus back on workflows .","title":"Data Science Bootstrap Notes"},{"location":"#how-to-organize-your-data-science-project","text":"Many years ago (in 2017, to be precise), I wrote down my first ideas on the theme of \"good data science project organization\". The result was a GitHub gist with a lot of ideas, but not automation provided.","title":"How to organize your data science project"},{"location":"#the-good-research-code-handbook","text":"This is an excellent resource that I got wind of in December 2021. In it is a detailed handbook-style resource that lays out step-by-step instructions for structuring your data science and/or research project code.","title":"The Good Research Code Handbook"},{"location":"#conversations-with-colleagues-at-moderna","text":"My conversations with colleagues on the DSAI team at Moderna were highly informative for this project.","title":"Conversations with colleagues at Moderna"},{"location":"cli/","text":"pyds Usage : $ pyds [ OPTIONS ] COMMAND [ ARGS ] ... Options : --install-completion : Install completion for the current shell. --show-completion : Show completion for the current shell, to copy it or customize the installation. --help : Show this message and exit. Commands : conda configure : Initial configuration for pyds. docs env package project system test : Run all tests in the project. pyds conda Usage : $ pyds conda [ OPTIONS ] COMMAND [ ARGS ] ... Options : --help : Show this message and exit. Commands : clean : Clean out your conda environment. rebuild : Rebuild the conda environment from scratch. update : Update the conda environment associated with... pyds conda clean Clean out your conda environment. Usage : $ pyds conda clean [ OPTIONS ] Options : --help : Show this message and exit. pyds conda rebuild Rebuild the conda environment from scratch. Usage : $ pyds conda rebuild [ OPTIONS ] Options : --help : Show this message and exit. pyds conda update Update the conda environment associated with the project. Usage : $ pyds conda update [ OPTIONS ] Options : --help : Show this message and exit. pyds configure Initial configuration for pyds. :param name: Your name. :param email: Your email address. :param github_username: Your GitHub username. :param twitter_username: Your Twitter username. :param linkedin_username: Your LinkedIn username. Usage : $ pyds configure [ OPTIONS ] Options : --name TEXT : Your name [required] --email TEXT : Your email address [required] --github-username TEXT : Your GitHub username [default: ] --twitter-username TEXT : Your Twitter username [default: ] --linkedin-username TEXT : Your LinkedIn username [default: ] --help : Show this message and exit. pyds docs Usage : $ pyds docs [ OPTIONS ] COMMAND [ ARGS ] ... Options : --help : Show this message and exit. Commands : build : Build docs for the project. serve : Serve docs for the project. pyds docs build Build docs for the project. Usage : $ pyds docs build [ OPTIONS ] Options : --help : Show this message and exit. pyds docs serve Serve docs for the project. Usage : $ pyds docs serve [ OPTIONS ] Options : --help : Show this message and exit. pyds env Usage : $ pyds env [ OPTIONS ] COMMAND [ ARGS ] ... Options : --help : Show this message and exit. Commands : delete-env-var : Remove an environment variable from the... set-env-var : Set a key-value pair in the .env file. show-env-vars : Show all environment variables. pyds env delete-env-var Remove an environment variable from the .env file. :param key: The name of the environment variable. Usage : $ pyds env delete-env-var [ OPTIONS ] KEY Arguments : KEY : [required] Options : --help : Show this message and exit. pyds env set-env-var Set a key-value pair in the .env file. :param key: The name of the environment variable. :param value: The value to set the environment variable to. Usage : $ pyds env set-env-var [ OPTIONS ] KEY VALUE Arguments : KEY : [required] VALUE : [required] Options : --help : Show this message and exit. pyds env show-env-vars Show all environment variables. :param keys: Whether to show the keys or not. :param values: Whether to show the values or not. Usage : $ pyds env show-env-vars [ OPTIONS ] Options : --keys / --no-keys : [default: True] --values / --no-values : [default: False] --help : Show this message and exit. pyds package Usage : $ pyds package [ OPTIONS ] COMMAND [ ARGS ] ... Options : --help : Show this message and exit. Commands : publish : Publish the custom package to a... reinstall : Reinstall the custom package into the conda... pyds package publish Publish the custom package to a pip-compatible server. :param bump: The part of the version number to bump. :param to: The name of the pip server on which to publish the package. Should be configured in your .pypirc. Can be run from anywhere within the project directory. Defaults to 'pypi'. :param dry_run: Whether you want to do just a dry-run. :raises FileNotFoundError: if the PyPI configuration file is not found. Usage : $ pyds package publish [ OPTIONS ] Options : --bump [major|minor|patch] : Is this a 'major', 'minor', or 'patch' release? [required] --to TEXT : The name of the pip server on which to publish the package. Should be configured in your .pypirc. [default: pypi] --dry-run / --no-dry-run : Whether this is a dry-run or not. [default: True] --help : Show this message and exit. pyds package reinstall Reinstall the custom package into the conda environment. :param env_file: The filename of the conda environment file. Defaults to environment.yml . Usage : $ pyds package reinstall [ OPTIONS ] Options : --env-file TEXT : Environment file name. [default: environment.yml] --help : Show this message and exit. pyds project Usage : $ pyds project [ OPTIONS ] COMMAND [ ARGS ] ... Options : --help : Show this message and exit. Commands : initialize : Initialize a new Python data science project. minitialize : Generate minimal scratch-like environment for... pyds project initialize Initialize a new Python data science project. :param project_name: Name of the new project to create. Becomes the directory name, kebab-cased, and custom source name, snake_cased. :param project_description: A one-line description of the project. :param license: The license to use for your new project. Defaults to \"MIT\". :param auto_create_env: Whether or not to automatically create a new conda environment for the project. Defaults to True. :param auto_jupyter_kernel: Whether or not to automatically expose the new Python environment to Jupyter as a Jupyter kernel. Defaults to True. :param auto_pre_commit: Whether or not to automatically install the pre-commit hooks. Defaults to True. Usage : $ pyds project initialize [ OPTIONS ] Options : --project-name TEXT : The project name. Will be snake-cased. Defaults to current working directory. [default: .] --project-description TEXT : A one-line description of your project. [required] --license TEXT : Your project's license. [default: MIT] --auto-create-env / --no-auto-create-env : Automatically create environment [default: True] --auto-jupyter-kernel / --no-auto-jupyter-kernel : Automatically expose environment kernel to Jupyter [default: True] --auto-pre-commit / --no-auto-pre-commit : Automatically install pre-commit [default: True] --help : Show this message and exit. pyds project minitialize Generate minimal scratch-like environment for prototyping purposes. This initializes git , source directory, tests, docs. Conda environment is created, package still installed into environment, implying setup.cfg and setup.py. We omit: - config files - pre-commit - devcontainer - .github :param project_name: Name of the new project to create. Becomes the directory name, kebab-cased, and custom source name, snake_cased. Usage : $ pyds project minitialize [ OPTIONS ] Options : --project-name TEXT : The project name. Will be snake-cased. Defaults to current working directory. [default: .] --help : Show this message and exit. pyds system Usage : $ pyds system [ OPTIONS ] COMMAND [ ARGS ] ... Options : --help : Show this message and exit. Commands : init : Bootstrap user's system with necessary... status : Report status for tools that we expect to... pyds system init Bootstrap user's system with necessary programs. Usage : $ pyds system init [ OPTIONS ] Options : --help : Show this message and exit. pyds system status Report status for tools that we expect to have installed. We check for the presence of: A conda installation. A homebrew installation. The presence of a .pypirc file. Usage : $ pyds system status [ OPTIONS ] Options : --help : Show this message and exit. pyds test Run all tests in the project. Usage : $ pyds test [ OPTIONS ] Options : --help : Show this message and exit.","title":"`pyds`"},{"location":"cli/#pyds","text":"Usage : $ pyds [ OPTIONS ] COMMAND [ ARGS ] ... Options : --install-completion : Install completion for the current shell. --show-completion : Show completion for the current shell, to copy it or customize the installation. --help : Show this message and exit. Commands : conda configure : Initial configuration for pyds. docs env package project system test : Run all tests in the project.","title":"pyds"},{"location":"cli/#pyds-conda","text":"Usage : $ pyds conda [ OPTIONS ] COMMAND [ ARGS ] ... Options : --help : Show this message and exit. Commands : clean : Clean out your conda environment. rebuild : Rebuild the conda environment from scratch. update : Update the conda environment associated with...","title":"pyds conda"},{"location":"cli/#pyds-conda-clean","text":"Clean out your conda environment. Usage : $ pyds conda clean [ OPTIONS ] Options : --help : Show this message and exit.","title":"pyds conda clean"},{"location":"cli/#pyds-conda-rebuild","text":"Rebuild the conda environment from scratch. Usage : $ pyds conda rebuild [ OPTIONS ] Options : --help : Show this message and exit.","title":"pyds conda rebuild"},{"location":"cli/#pyds-conda-update","text":"Update the conda environment associated with the project. Usage : $ pyds conda update [ OPTIONS ] Options : --help : Show this message and exit.","title":"pyds conda update"},{"location":"cli/#pyds-configure","text":"Initial configuration for pyds. :param name: Your name. :param email: Your email address. :param github_username: Your GitHub username. :param twitter_username: Your Twitter username. :param linkedin_username: Your LinkedIn username. Usage : $ pyds configure [ OPTIONS ] Options : --name TEXT : Your name [required] --email TEXT : Your email address [required] --github-username TEXT : Your GitHub username [default: ] --twitter-username TEXT : Your Twitter username [default: ] --linkedin-username TEXT : Your LinkedIn username [default: ] --help : Show this message and exit.","title":"pyds configure"},{"location":"cli/#pyds-docs","text":"Usage : $ pyds docs [ OPTIONS ] COMMAND [ ARGS ] ... Options : --help : Show this message and exit. Commands : build : Build docs for the project. serve : Serve docs for the project.","title":"pyds docs"},{"location":"cli/#pyds-docs-build","text":"Build docs for the project. Usage : $ pyds docs build [ OPTIONS ] Options : --help : Show this message and exit.","title":"pyds docs build"},{"location":"cli/#pyds-docs-serve","text":"Serve docs for the project. Usage : $ pyds docs serve [ OPTIONS ] Options : --help : Show this message and exit.","title":"pyds docs serve"},{"location":"cli/#pyds-env","text":"Usage : $ pyds env [ OPTIONS ] COMMAND [ ARGS ] ... Options : --help : Show this message and exit. Commands : delete-env-var : Remove an environment variable from the... set-env-var : Set a key-value pair in the .env file. show-env-vars : Show all environment variables.","title":"pyds env"},{"location":"cli/#pyds-env-delete-env-var","text":"Remove an environment variable from the .env file. :param key: The name of the environment variable. Usage : $ pyds env delete-env-var [ OPTIONS ] KEY Arguments : KEY : [required] Options : --help : Show this message and exit.","title":"pyds env delete-env-var"},{"location":"cli/#pyds-env-set-env-var","text":"Set a key-value pair in the .env file. :param key: The name of the environment variable. :param value: The value to set the environment variable to. Usage : $ pyds env set-env-var [ OPTIONS ] KEY VALUE Arguments : KEY : [required] VALUE : [required] Options : --help : Show this message and exit.","title":"pyds env set-env-var"},{"location":"cli/#pyds-env-show-env-vars","text":"Show all environment variables. :param keys: Whether to show the keys or not. :param values: Whether to show the values or not. Usage : $ pyds env show-env-vars [ OPTIONS ] Options : --keys / --no-keys : [default: True] --values / --no-values : [default: False] --help : Show this message and exit.","title":"pyds env show-env-vars"},{"location":"cli/#pyds-package","text":"Usage : $ pyds package [ OPTIONS ] COMMAND [ ARGS ] ... Options : --help : Show this message and exit. Commands : publish : Publish the custom package to a... reinstall : Reinstall the custom package into the conda...","title":"pyds package"},{"location":"cli/#pyds-package-publish","text":"Publish the custom package to a pip-compatible server. :param bump: The part of the version number to bump. :param to: The name of the pip server on which to publish the package. Should be configured in your .pypirc. Can be run from anywhere within the project directory. Defaults to 'pypi'. :param dry_run: Whether you want to do just a dry-run. :raises FileNotFoundError: if the PyPI configuration file is not found. Usage : $ pyds package publish [ OPTIONS ] Options : --bump [major|minor|patch] : Is this a 'major', 'minor', or 'patch' release? [required] --to TEXT : The name of the pip server on which to publish the package. Should be configured in your .pypirc. [default: pypi] --dry-run / --no-dry-run : Whether this is a dry-run or not. [default: True] --help : Show this message and exit.","title":"pyds package publish"},{"location":"cli/#pyds-package-reinstall","text":"Reinstall the custom package into the conda environment. :param env_file: The filename of the conda environment file. Defaults to environment.yml . Usage : $ pyds package reinstall [ OPTIONS ] Options : --env-file TEXT : Environment file name. [default: environment.yml] --help : Show this message and exit.","title":"pyds package reinstall"},{"location":"cli/#pyds-project","text":"Usage : $ pyds project [ OPTIONS ] COMMAND [ ARGS ] ... Options : --help : Show this message and exit. Commands : initialize : Initialize a new Python data science project. minitialize : Generate minimal scratch-like environment for...","title":"pyds project"},{"location":"cli/#pyds-project-initialize","text":"Initialize a new Python data science project. :param project_name: Name of the new project to create. Becomes the directory name, kebab-cased, and custom source name, snake_cased. :param project_description: A one-line description of the project. :param license: The license to use for your new project. Defaults to \"MIT\". :param auto_create_env: Whether or not to automatically create a new conda environment for the project. Defaults to True. :param auto_jupyter_kernel: Whether or not to automatically expose the new Python environment to Jupyter as a Jupyter kernel. Defaults to True. :param auto_pre_commit: Whether or not to automatically install the pre-commit hooks. Defaults to True. Usage : $ pyds project initialize [ OPTIONS ] Options : --project-name TEXT : The project name. Will be snake-cased. Defaults to current working directory. [default: .] --project-description TEXT : A one-line description of your project. [required] --license TEXT : Your project's license. [default: MIT] --auto-create-env / --no-auto-create-env : Automatically create environment [default: True] --auto-jupyter-kernel / --no-auto-jupyter-kernel : Automatically expose environment kernel to Jupyter [default: True] --auto-pre-commit / --no-auto-pre-commit : Automatically install pre-commit [default: True] --help : Show this message and exit.","title":"pyds project initialize"},{"location":"cli/#pyds-project-minitialize","text":"Generate minimal scratch-like environment for prototyping purposes. This initializes git , source directory, tests, docs. Conda environment is created, package still installed into environment, implying setup.cfg and setup.py. We omit: - config files - pre-commit - devcontainer - .github :param project_name: Name of the new project to create. Becomes the directory name, kebab-cased, and custom source name, snake_cased. Usage : $ pyds project minitialize [ OPTIONS ] Options : --project-name TEXT : The project name. Will be snake-cased. Defaults to current working directory. [default: .] --help : Show this message and exit.","title":"pyds project minitialize"},{"location":"cli/#pyds-system","text":"Usage : $ pyds system [ OPTIONS ] COMMAND [ ARGS ] ... Options : --help : Show this message and exit. Commands : init : Bootstrap user's system with necessary... status : Report status for tools that we expect to...","title":"pyds system"},{"location":"cli/#pyds-system-init","text":"Bootstrap user's system with necessary programs. Usage : $ pyds system init [ OPTIONS ] Options : --help : Show this message and exit.","title":"pyds system init"},{"location":"cli/#pyds-system-status","text":"Report status for tools that we expect to have installed. We check for the presence of: A conda installation. A homebrew installation. The presence of a .pypirc file. Usage : $ pyds system status [ OPTIONS ] Options : --help : Show this message and exit.","title":"pyds system status"},{"location":"cli/#pyds-test","text":"Run all tests in the project. Usage : $ pyds test [ OPTIONS ] Options : --help : Show this message and exit.","title":"pyds test"},{"location":"contributing/00-index/","text":"Contributing There are many ways to contribute to the project. Please read the next few pages to explore how!","title":"Contributing"},{"location":"contributing/00-index/#contributing","text":"There are many ways to contribute to the project. Please read the next few pages to explore how!","title":"Contributing"},{"location":"contributing/01-new-commands/","text":"Contributing new commands to the public package PyDS is intentionally targeting the broadest possible audience. Therefore, idiosyncratic workflows are intentionally out-of-scope. The issue tracker is the place to propose new workflows. There should be at least 50 thumbs up on the proposed new command and its underlying shell implementation. New workflows should satisfy the design philosophy of the package. They should also be well-established workflows, with arguments presented why they are well-established. (Evidence may include a smattering of blog posts introducing \"how to do X\", official documentation present on the tool, and )","title":"Contributing new commands to the public package"},{"location":"contributing/01-new-commands/#contributing-new-commands-to-the-public-package","text":"PyDS is intentionally targeting the broadest possible audience. Therefore, idiosyncratic workflows are intentionally out-of-scope. The issue tracker is the place to propose new workflows. There should be at least 50 thumbs up on the proposed new command and its underlying shell implementation. New workflows should satisfy the design philosophy of the package. They should also be well-established workflows, with arguments presented why they are well-established. (Evidence may include a smattering of blog posts introducing \"how to do X\", official documentation present on the tool, and )","title":"Contributing new commands to the public package"},{"location":"contributing/02-documentation/","text":"Documentation We'd love help with documentation! Asciinema recordings What are the workflows that you love the most? Record them down using asciinema ! Missing docs Did you find something not documented in the docs that should be? Raise the issue in the Issue Tracker , and let's find a good home for it together.","title":"Documentation"},{"location":"contributing/02-documentation/#documentation","text":"We'd love help with documentation!","title":"Documentation"},{"location":"contributing/02-documentation/#asciinema-recordings","text":"What are the workflows that you love the most? Record them down using asciinema !","title":"Asciinema recordings"},{"location":"contributing/02-documentation/#missing-docs","text":"Did you find something not documented in the docs that should be? Raise the issue in the Issue Tracker , and let's find a good home for it together.","title":"Missing docs"},{"location":"design/00-index/","text":"Design Philosophy PyDS wraps workflows first; tools are also wrapped, but as a matter of convenience. Workflows are defined here as a chain of shell commands, which might involve multiple tools, that are repetitively executed. You might be tempted to put those into a Makefile, and execute them with a single command, make something . That's the kind of workflow that PyDS wraps. Within a Python data science project, there are workflows that can be automated easily. These workflows may involve multiple commands; that's extra headspace to commit to memory. Here are some workflows supported. Because workflows are verbs , PyDS' internal sub-command structure is also centered around verbs. Doing so makes expressing what we want to do much more natural. Example commands look like: pyds project initialize pyds package publish pyds docs preview And more generically: pyds <thing> <verb> Here, <thing> refers to artifacts of some kind. Documentation ( docs ) are an artifact of the project that we make. The Python package ( package ) is another artifact. The project as a whole ( project ) is yet another. We try to avoid anti-patterns in the implementation. There is no lock-in with pyds. We don't implement our own idiosyncratic tools; literally most of the commands dispatch out to other tools that we assume to be present on the user's system. All that pyds does is chain them together into higher-order workflows that follow a natural English expression. We want to support workflows that compose together tools in the stack; we don't want to own the stack.","title":"Design Philosophy"},{"location":"design/00-index/#design-philosophy","text":"PyDS wraps workflows first; tools are also wrapped, but as a matter of convenience. Workflows are defined here as a chain of shell commands, which might involve multiple tools, that are repetitively executed. You might be tempted to put those into a Makefile, and execute them with a single command, make something . That's the kind of workflow that PyDS wraps. Within a Python data science project, there are workflows that can be automated easily. These workflows may involve multiple commands; that's extra headspace to commit to memory. Here are some workflows supported. Because workflows are verbs , PyDS' internal sub-command structure is also centered around verbs. Doing so makes expressing what we want to do much more natural. Example commands look like: pyds project initialize pyds package publish pyds docs preview And more generically: pyds <thing> <verb> Here, <thing> refers to artifacts of some kind. Documentation ( docs ) are an artifact of the project that we make. The Python package ( package ) is another artifact. The project as a whole ( project ) is yet another. We try to avoid anti-patterns in the implementation. There is no lock-in with pyds. We don't implement our own idiosyncratic tools; literally most of the commands dispatch out to other tools that we assume to be present on the user's system. All that pyds does is chain them together into higher-order workflows that follow a natural English expression. We want to support workflows that compose together tools in the stack; we don't want to own the stack.","title":"Design Philosophy"},{"location":"design/01-opinionated-choices/","text":"Opinionated choices Dependence on conda The Anaconda distribution of Python has become the de facto Python distribution recommended for data scientists to use. Embracing software development practices I strongly believe that models, at their core, are software. Hence, workflows commonly associated with software development, such as writing tests and documentation, ought to be part of a data scientist's workflow as well. As data scientists, if we don't embrace software development ideas, we set up future personal headache and barriers to current collaboration. Software development workflows heavily relies on the concept of single sources of truth for stuff . This is the spirit behind the term refactoring , where we extract out stuff that might have been copied-and-pasted. That stuff might be a function that, after refactoring, we can import and use elsewhere. It might be a Markdown document that contains units of ideas that we refer to elsewhere. Or, it might a reference Jupyter notebook that teaches us the linearized logic of the analysis; in other words, an authoritative \"report\" that forms the basis of our knowledge sharing efforts. Buying in to \"authoritative single sources of truth\" are what prevents us from sloppily duplicating notebooks, copying and pasting code, and creating a world of confusion for future selves and collaborators. Notebooks are best used for specific purposes From personal experience and from reading others' experiences, we've come to the conclusion that notebooks are best used as A space for experimentation and prototyping, and Providing documentation for others. Usage in any other form makes us more prone to sloppy workflows. (To be clear, I'm not saying that we are guaranteed to engage in sloppy workflows, I am merely saying that we will end up with fewer mental guardrails against sloppy workflows.) Custom source code library Based on the philosophy of \"single sources of truth\", PyDS CLI automatically creates a custom source code library that is automatically installed into the automatically-created conda environment that is also automatically named in a way that enables you to import into notebooks and other source code. By default, this is done as a local, editable install, enabling you to freely modify the source code and use the new code automatically without needing to re-install the package each time you make changes. This source code library is a great place to refactor out code! For example, if you write a custom PyMC3 or JAX model, you can stick it into models.py . If you have data preprocessing code, you can stick them into preprocessing.py . And if you want to automatically validate dataframes, write a pandera schema and stick it in schemas.py ! Testing with pytest Software testing gives you a contract between current self and others. By writing a test down, you're recording what you expect to be the behaviour of a function at the time of implementation. If in the future someone changes the function or changes something that changes the expected inputs of that function, then a test will help you catch deviations from those expectations. pytest is the modern way to handle software testing, by abstracting away lots of boilerplate that would otherwise be written using the built-in unittest Python library. Docs with mkdocs MkDocs is a growing and popular alternative to Sphinx, which has historically been the backbone of Python project documentation. Being able to use pure Markdown to write docs confers a lot of advantages, but accessibility and ease-of-use is its biggest advantage: Its syntax is widely-known, it is easy for newcomers to pick up, one rarely needs to refer to Markdown documentation to look up syntax, it is easily portable to other formats (PDF, LaTeX, HTML, etc.), and for web-targeted documentation, advanced users can easily mix in custom HTML as necessary. Building docs with Jupyter notebooks Jupyter notebooks are awesome for prototyping and documenting. By including mknotebooks automatically, we give ourselves the superpower of being able to include Jupyter notebooks inside our docs. If paired with a CI/CD bot that always executes notebooks that are part of docs , then suddenly, our docs are living docs, and can effectively serve as an 'integration test' for our analysis code. Data never live in the repo Data should never live in the repository. Data should never live in the repository. Data should never, ever live in the repository. In a cloud-first world, we should be able to reliably store files in a remote location and pull them into our machine on demand. Leverage CI/CD as a bot CI/CD gets bandied about as a fancy term in some circles. I tend to think of CI/CD in a much, much more simple fashion: it's a robot. \ud83d\ude05 CI/CD systems, such as GitHub Actions, are robots that you can program using YAML (or, gasp! Jenkins) files rather than Bash scripts alone. We provide some basic CI/CD files in the form of GitHub Actions configuration files that automatically provide code checking, testing, and docs building on GitHub. GitHub Actions We target GitHub actions as an opinionated choice. GitHub Actions hits the right level of abstraction for CI/CD pipelining. The syntax is easy to learn, it has easily mix-and-matchable components, and is free for open source projects. PyDS CLI ships with GitHub actions that will help you check your code style and automatically run tests; you don't have to remember to add them in! (But feel free to delete these files if they're not necessary; they're generally harmless outside of GitHub so we don't bother asking if you want them or not and instead leave them to you to remove.) Pre-commit hooks We primarily use pre-commit hooks to help you catch code quality issues before you git commit them. We use the pre-commit framework to manage the code checking tools. You can think of pre-commit as a bot that runs code checks before you're allowed to commit the code; if any fails, you'll be alerted. We use checkers for docstring coverage, docstring argument coverage, code formatting, and ensuring notebook outputs are never committed. Using pre-commit can be a bit jarring at first, because you might have a ton of work being committed. That said, though, if you commit often and early, then you'll catch your code quality issues on a faster turnaround cycle, which makes fixing code issues much easier than otherwise. Development Containers Development containers are another innovation in the developer tooling space that gives us the ability to work in an even more isolated environment than pure conda environments alone. If we were to use Russian Matryoshka dolls (the nested dolls) as an analogy, Docker-based development containers are the largest doll that ship a computer, while conda environments are the second largest doll that ship an isolated Python interpreter. Development containers work well with VSCode; we ship an opinionated VSCode dev container Dockerfile definition that VSCode can automatically discover and build for you. If dev containers and, more broadly, GitHub codespaces are something you use, then you'll have no barriers to starting them up.","title":"Opinionated choices"},{"location":"design/01-opinionated-choices/#opinionated-choices","text":"","title":"Opinionated choices"},{"location":"design/01-opinionated-choices/#dependence-on-conda","text":"The Anaconda distribution of Python has become the de facto Python distribution recommended for data scientists to use.","title":"Dependence on conda"},{"location":"design/01-opinionated-choices/#embracing-software-development-practices","text":"I strongly believe that models, at their core, are software. Hence, workflows commonly associated with software development, such as writing tests and documentation, ought to be part of a data scientist's workflow as well. As data scientists, if we don't embrace software development ideas, we set up future personal headache and barriers to current collaboration. Software development workflows heavily relies on the concept of single sources of truth for stuff . This is the spirit behind the term refactoring , where we extract out stuff that might have been copied-and-pasted. That stuff might be a function that, after refactoring, we can import and use elsewhere. It might be a Markdown document that contains units of ideas that we refer to elsewhere. Or, it might a reference Jupyter notebook that teaches us the linearized logic of the analysis; in other words, an authoritative \"report\" that forms the basis of our knowledge sharing efforts. Buying in to \"authoritative single sources of truth\" are what prevents us from sloppily duplicating notebooks, copying and pasting code, and creating a world of confusion for future selves and collaborators.","title":"Embracing software development practices"},{"location":"design/01-opinionated-choices/#notebooks-are-best-used-for-specific-purposes","text":"From personal experience and from reading others' experiences, we've come to the conclusion that notebooks are best used as A space for experimentation and prototyping, and Providing documentation for others. Usage in any other form makes us more prone to sloppy workflows. (To be clear, I'm not saying that we are guaranteed to engage in sloppy workflows, I am merely saying that we will end up with fewer mental guardrails against sloppy workflows.)","title":"Notebooks are best used for specific purposes"},{"location":"design/01-opinionated-choices/#custom-source-code-library","text":"Based on the philosophy of \"single sources of truth\", PyDS CLI automatically creates a custom source code library that is automatically installed into the automatically-created conda environment that is also automatically named in a way that enables you to import into notebooks and other source code. By default, this is done as a local, editable install, enabling you to freely modify the source code and use the new code automatically without needing to re-install the package each time you make changes. This source code library is a great place to refactor out code! For example, if you write a custom PyMC3 or JAX model, you can stick it into models.py . If you have data preprocessing code, you can stick them into preprocessing.py . And if you want to automatically validate dataframes, write a pandera schema and stick it in schemas.py !","title":"Custom source code library"},{"location":"design/01-opinionated-choices/#testing-with-pytest","text":"Software testing gives you a contract between current self and others. By writing a test down, you're recording what you expect to be the behaviour of a function at the time of implementation. If in the future someone changes the function or changes something that changes the expected inputs of that function, then a test will help you catch deviations from those expectations. pytest is the modern way to handle software testing, by abstracting away lots of boilerplate that would otherwise be written using the built-in unittest Python library.","title":"Testing with pytest"},{"location":"design/01-opinionated-choices/#docs-with-mkdocs","text":"MkDocs is a growing and popular alternative to Sphinx, which has historically been the backbone of Python project documentation. Being able to use pure Markdown to write docs confers a lot of advantages, but accessibility and ease-of-use is its biggest advantage: Its syntax is widely-known, it is easy for newcomers to pick up, one rarely needs to refer to Markdown documentation to look up syntax, it is easily portable to other formats (PDF, LaTeX, HTML, etc.), and for web-targeted documentation, advanced users can easily mix in custom HTML as necessary.","title":"Docs with mkdocs"},{"location":"design/01-opinionated-choices/#building-docs-with-jupyter-notebooks","text":"Jupyter notebooks are awesome for prototyping and documenting. By including mknotebooks automatically, we give ourselves the superpower of being able to include Jupyter notebooks inside our docs. If paired with a CI/CD bot that always executes notebooks that are part of docs , then suddenly, our docs are living docs, and can effectively serve as an 'integration test' for our analysis code.","title":"Building docs with Jupyter notebooks"},{"location":"design/01-opinionated-choices/#data-never-live-in-the-repo","text":"Data should never live in the repository. Data should never live in the repository. Data should never, ever live in the repository. In a cloud-first world, we should be able to reliably store files in a remote location and pull them into our machine on demand.","title":"Data never live in the repo"},{"location":"design/01-opinionated-choices/#leverage-cicd-as-a-bot","text":"CI/CD gets bandied about as a fancy term in some circles. I tend to think of CI/CD in a much, much more simple fashion: it's a robot. \ud83d\ude05 CI/CD systems, such as GitHub Actions, are robots that you can program using YAML (or, gasp! Jenkins) files rather than Bash scripts alone. We provide some basic CI/CD files in the form of GitHub Actions configuration files that automatically provide code checking, testing, and docs building on GitHub.","title":"Leverage CI/CD as a bot"},{"location":"design/01-opinionated-choices/#github-actions","text":"We target GitHub actions as an opinionated choice. GitHub Actions hits the right level of abstraction for CI/CD pipelining. The syntax is easy to learn, it has easily mix-and-matchable components, and is free for open source projects. PyDS CLI ships with GitHub actions that will help you check your code style and automatically run tests; you don't have to remember to add them in! (But feel free to delete these files if they're not necessary; they're generally harmless outside of GitHub so we don't bother asking if you want them or not and instead leave them to you to remove.)","title":"GitHub Actions"},{"location":"design/01-opinionated-choices/#pre-commit-hooks","text":"We primarily use pre-commit hooks to help you catch code quality issues before you git commit them. We use the pre-commit framework to manage the code checking tools. You can think of pre-commit as a bot that runs code checks before you're allowed to commit the code; if any fails, you'll be alerted. We use checkers for docstring coverage, docstring argument coverage, code formatting, and ensuring notebook outputs are never committed. Using pre-commit can be a bit jarring at first, because you might have a ton of work being committed. That said, though, if you commit often and early, then you'll catch your code quality issues on a faster turnaround cycle, which makes fixing code issues much easier than otherwise.","title":"Pre-commit hooks"},{"location":"design/01-opinionated-choices/#development-containers","text":"Development containers are another innovation in the developer tooling space that gives us the ability to work in an even more isolated environment than pure conda environments alone. If we were to use Russian Matryoshka dolls (the nested dolls) as an analogy, Docker-based development containers are the largest doll that ship a computer, while conda environments are the second largest doll that ship an isolated Python interpreter. Development containers work well with VSCode; we ship an opinionated VSCode dev container Dockerfile definition that VSCode can automatically discover and build for you. If dev containers and, more broadly, GitHub codespaces are something you use, then you'll have no barriers to starting them up.","title":"Development Containers"},{"location":"workflows/01-new-project/","text":"Creating a new data science project When creating a new data science project, there are a lot of little details to remember in order to set up a sane project structure that supports great practices. PyDS lets you set all of that up with one interactive command: pyds project initialize Behind the scenes, we create a new Python project with an opinionated directory structure. There's a place to put your notebooks/ . There's a custom source directory named after your project_name/ , where you can easily refactor out notebook code. Your project_name/ source also comes with basic command line interface capabilities that you can expand on in case you wish to make a CLI tool for others to use. There are a smattering of code quality tools automatically installed as part of your pre-commit hooks. You default to getting a conda environment automagically, and you have a custom Python package ( project_name/ ) installed into the environment too, ensuring portability of your data analysis code.","title":"Creating a new data science project"},{"location":"workflows/01-new-project/#creating-a-new-data-science-project","text":"When creating a new data science project, there are a lot of little details to remember in order to set up a sane project structure that supports great practices. PyDS lets you set all of that up with one interactive command: pyds project initialize Behind the scenes, we create a new Python project with an opinionated directory structure. There's a place to put your notebooks/ . There's a custom source directory named after your project_name/ , where you can easily refactor out notebook code. Your project_name/ source also comes with basic command line interface capabilities that you can expand on in case you wish to make a CLI tool for others to use. There are a smattering of code quality tools automatically installed as part of your pre-commit hooks. You default to getting a conda environment automagically, and you have a custom Python package ( project_name/ ) installed into the environment too, ensuring portability of your data analysis code.","title":"Creating a new data science project"},{"location":"workflows/02-minimal-project/","text":"Creating a minimal project Sometimes you don't need a full-fledged project, but a minimal one instead. In this case, we have a minitialize command! ( minitialize is a portmanteau of minimal and initialize .)","title":"Creating a minimal project"},{"location":"workflows/02-minimal-project/#creating-a-minimal-project","text":"Sometimes you don't need a full-fledged project, but a minimal one instead. In this case, we have a minitialize command! ( minitialize is a portmanteau of minimal and initialize .)","title":"Creating a minimal project"},{"location":"workflows/03-publishing-package/","text":"Publishing the custom source code to a pip server For a Python data science project, the work product might be a Python package that enables your work to be used by other developers, such as your engineering colleagues or other data scientists. To publish your package to your pre-configured internal pip server, you can execute the following interactive command: pyds package publish --to internal --bump patch Underneath the hood, you'd have to remember at least the following commands to do a new release: bumpversion patch --verbose rm dist/* python -m build . twine upload -r pip-internal dist/ That's a lot of jumping between tools! Instead of having to remember all of them, now you only have to remember pyds publish package .","title":"Publishing the custom source code to a pip server"},{"location":"workflows/03-publishing-package/#publishing-the-custom-source-code-to-a-pip-server","text":"For a Python data science project, the work product might be a Python package that enables your work to be used by other developers, such as your engineering colleagues or other data scientists. To publish your package to your pre-configured internal pip server, you can execute the following interactive command: pyds package publish --to internal --bump patch Underneath the hood, you'd have to remember at least the following commands to do a new release: bumpversion patch --verbose rm dist/* python -m build . twine upload -r pip-internal dist/ That's a lot of jumping between tools! Instead of having to remember all of them, now you only have to remember pyds publish package .","title":"Publishing the custom source code to a pip server"}]}